---
title: 'Laboratorio 6: Análisis de sentimientos'
author: "Grupo 7"
date: "9/10/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Librerías
```{r}
library("readr") #Lee los archivos 
library("tm")  #Contiene tranformaciones para el text mining
library("wordcloud")
library(ggplot2) #para gráficos
library("SentimentAnalysis")
```

# Descripción de los datos  
El dataset es un conjunto de reseñas de 1000 productos diferentes. El tamaño  del conjunto de datos es de 94 Megabytes, donde el conglomerado de datos se encuentra distribuido en un total de 25 columnas. 

El origen de la base de datos proviene de Datafiniti´s Product database e incluye datos de la reseña como el producto, el autor de la reseña, el título, la fecha, etc. 

El tipo de corpus es general dado que contiene diversos ejemplos del habla en idioma inglés que ha sido producido por personas de distintas edades, regiones y clases sociales. Así también se podría calificar como monolingue.
```{r}
#-------------------------------------------------
# Directorios para de cada integrante del grupo

#/Users/odalisrg/Documents/Semestre 6/Data Science/Laboratorio6DS
#/Users/quiebres/Documents/Ivan Maldonado/UVG/Sexto Semestre/Data Science/Laboratorio6DS
#-------------------------------------------------

# Se lee y se crea un dataframe del archivo
setwd("/Users/odalisrg/Documents/Semestre 6/Data Science/Laboratorio6DS")
productos <- read_csv("GrammarandProductReviews.csv")
class(productos)
```




# Limpieza y preprocesamiento
Antes de comenzar a realizar la limpieza y preprocesamiento de los datos, fue necesario separar

## Creación del corpus
Para limpiar el texto de las opiniones de los productos realizadas por los clientes, decidimos solo tomar en cuenta la columna de reviews.text, ya que en esta parte del dataset se conglomeran las opiniones de los clientes. De esta manera, 
```{r}
texto <- paste(productos$reviews.title, collapse = " ")
opinion <- Corpus(VectorSource(texto))
```

## Limpieza y preprocesamiento del corpus
Para este apartado, se decidió transformar todas las letras mayúsculas a minúsculas para obtener un corpus más homogéneo. Con respecto a los números y a los stopwords, también se decidió eliminarlos ya que interferían en el análisis de palabras y facilitan a que el corpus reduzca su tamaño. Ahora bien, con los signos de puntuación se decidió eliminarlos todos, incluyendo aquellas combinaciones de signos que crean un emoticon. Decidimos basarnos solamente en la palabra y en su contexto dentro del corpus. 
```{r}
opinion <- tm_map(opinion, content_transformer(tolower))
opinion <- tm_map(opinion, content_transformer(removeNumbers))
opinion <- tm_map(opinion, content_transformer(removePunctuation))
opinion <- tm_map(opinion, stripWhitespace)
opinion <- tm_map(opinion, removeWords,stopwords(kind="en"))
```



# Análisis exploratorio

## Creación de matriz de documentación
```{r}
opinion_dtm <- DocumentTermMatrix(opinion)
```

## Encontrando la palabras con más frecuencia
```{r}
findFreqTerms(opinion_dtm, lowfreq=1000)
```

## Nube de palabas
Al momento de realizar la nube de palabras, vemos que las que más resaltan son palabras positivas, tales como _great, love, good, awesome, best_. También es importante mencionar que las palabras que están en color verde, tienen a mencoinar un poco más del producto que se está evaluando.
```{r}
wordcloud(opinion, max.words=100, random.order= FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```
## Histograma de frecuencias
Este histograma muestra aquellas palabras que tienen por lo menos 1,500 repeticiones. Vemos que vuelven aparecer comentarios positivos, lo cual puede mostrar un indicio que la mayoría de opiniones que han realizado los clientes han sido positivas. Cabe mencionar que hace falta indagar más acera de las opiniones negativas.
```{r}
freq.reviews <- colSums(as.matrix(opinion_dtm))

wfReviews <- data.frame(word = names(freq.reviews), freq = freq.reviews)
head(wfReviews)


HistoR <- ggplot(subset(wfReviews, freq>1500), aes(x = reorder(word, -freq), y = freq)) +
          geom_bar(stat = "identity") + ggtitle("Palabras más frecuentes") +
          theme(axis.text.x=element_text(angle=45, hjust=1))
HistoR

```

# Análisis de sentimientos
```{r}
a_sent<-analyzeSentiment(opinion_dtm) 
b_sent<-convertToBinaryResponse(a_sent)
productos<-cbind(productos, b_sent)
```












